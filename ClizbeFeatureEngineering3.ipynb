{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ClizbeFeatureEngineering3.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyO1dNP9ffgWY4643XU+R2ZZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/davidclizbe/datascience/blob/master/ClizbeFeatureEngineering3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "alESkt7lJIu_"
      },
      "source": [
        "Train your own word2vec representations as we did in our first example in the checkpoint. But, you need to experiment with the hyperparameters of the vectorization step. Modify the hyperparameters and run the classification models again. Can you wrangle any improvements?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4a2dbgBeI_Ql",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 336
        },
        "outputId": "29a6675a-c412-4b94-a4c3-73636def97f1"
      },
      "source": [
        "pip install gensim"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gensim in /usr/local/lib/python3.6/dist-packages (3.6.0)\n",
            "Requirement already satisfied: numpy>=1.11.3 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.18.5)\n",
            "Requirement already satisfied: six>=1.5.0 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.15.0)\n",
            "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.4.1)\n",
            "Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.6/dist-packages (from gensim) (2.1.0)\n",
            "Requirement already satisfied: boto in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim) (2.49.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim) (2.23.0)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim) (1.14.33)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim) (2020.6.20)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim) (1.24.3)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.2.1->gensim) (0.10.0)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.2.1->gensim) (0.3.3)\n",
            "Requirement already satisfied: botocore<1.18.0,>=1.17.33 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.2.1->gensim) (1.17.33)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.18.0,>=1.17.33->boto3->smart-open>=1.2.1->gensim) (0.15.2)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.18.0,>=1.17.33->boto3->smart-open>=1.2.1->gensim) (2.8.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uZVKAun2JX_z",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 547
        },
        "outputId": "98a9e76a-2d0b-4225-fabb-ed481208ec68"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import sklearn\n",
        "import spacy\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import gutenberg\n",
        "import gensim\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "nltk.download('gutenberg')\n",
        "!python -m spacy download en"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/gutenberg.zip.\n",
            "Requirement already satisfied: en_core_web_sm==2.2.5 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.5/en_core_web_sm-2.2.5.tar.gz#egg=en_core_web_sm==2.2.5 in /usr/local/lib/python3.6/dist-packages (2.2.5)\n",
            "Requirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.6/dist-packages (from en_core_web_sm==2.2.5) (2.2.4)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.18.5)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.2)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.4.1)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.1.3)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.23.0)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.7.1)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.2)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (4.41.1)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (7.4.0)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.0.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (49.2.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2020.6.20)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (1.7.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.1.0)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;2m✔ Linking successful\u001b[0m\n",
            "/usr/local/lib/python3.6/dist-packages/en_core_web_sm -->\n",
            "/usr/local/lib/python3.6/dist-packages/spacy/data/en\n",
            "You can now load the model via spacy.load('en')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ne02ZRKUJa-i"
      },
      "source": [
        "# utility function for standard text cleaning\n",
        "def text_cleaner(text):\n",
        "    # visual inspection identifies a form of punctuation spaCy does not\n",
        "    # recognize: the double dash '--'.  Better get rid of it now!\n",
        "    text = re.sub(r'--',' ',text)\n",
        "    text = re.sub(\"[\\[].*?[\\]]\", \"\", text)\n",
        "    text = re.sub(r\"(\\b|\\s+\\-?|^\\-?)(\\d+|\\d*\\.\\d+)\\b\", \" \", text)\n",
        "    text = ' '.join(text.split())\n",
        "    return text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qFPKNHxKJdyU"
      },
      "source": [
        "# load and clean the data\n",
        "persuasion = gutenberg.raw('austen-persuasion.txt')\n",
        "alice = gutenberg.raw('carroll-alice.txt')\n",
        "\n",
        "# the chapter indicator is idiosyncratic\n",
        "persuasion = re.sub(r'Chapter \\d+', '', persuasion)\n",
        "alice = re.sub(r'CHAPTER .*', '', alice)\n",
        "    \n",
        "alice = text_cleaner(alice)\n",
        "persuasion = text_cleaner(persuasion)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1D6ut4WXJh8I"
      },
      "source": [
        "# parse the cleaned novels. This can take a bit.\n",
        "nlp = spacy.load('en')\n",
        "alice_doc = nlp(alice)\n",
        "persuasion_doc = nlp(persuasion)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L1vZmbrbJkh2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 203
        },
        "outputId": "203e126d-73d7-4dfc-f212-a8d3987035ee"
      },
      "source": [
        "# group into sentences\n",
        "alice_sents = [[sent, \"Carroll\"] for sent in alice_doc.sents]\n",
        "persuasion_sents = [[sent, \"Austen\"] for sent in persuasion_doc.sents]\n",
        "\n",
        "# combine the sentences from the two novels into one data frame\n",
        "sentences = pd.DataFrame(alice_sents + persuasion_sents, columns = [\"text\", \"author\"])\n",
        "sentences.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>author</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>(Alice, was, beginning, to, get, very, tired, ...</td>\n",
              "      <td>Carroll</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>(So, she, was, considering, in, her, own, mind...</td>\n",
              "      <td>Carroll</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>(There, was, nothing, so, VERY, remarkable, in...</td>\n",
              "      <td>Carroll</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>(Oh, dear, !)</td>\n",
              "      <td>Carroll</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>(I, shall, be, late, !, ')</td>\n",
              "      <td>Carroll</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                text   author\n",
              "0  (Alice, was, beginning, to, get, very, tired, ...  Carroll\n",
              "1  (So, she, was, considering, in, her, own, mind...  Carroll\n",
              "2  (There, was, nothing, so, VERY, remarkable, in...  Carroll\n",
              "3                                      (Oh, dear, !)  Carroll\n",
              "4                         (I, shall, be, late, !, ')  Carroll"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cARnkOX3JqXZ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 326
        },
        "outputId": "e77317f4-26b1-449a-f9df-2f1bb8ebf8fd"
      },
      "source": [
        "# get rid off stop words and punctuation\n",
        "# and lemmatize the tokens\n",
        "for i, sentence in enumerate(sentences[\"text\"]):\n",
        "    sentences.loc[i, \"text\"] = [token.lemma_ for token in sentence if not token.is_punct and not token.is_stop]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-5196f3ea1f93>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# and lemmatize the tokens\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentence\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"text\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0msentences\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"text\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlemma_\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msentence\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtoken\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_punct\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtoken\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_stop\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-10-5196f3ea1f93>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# and lemmatize the tokens\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentence\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"text\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0msentences\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"text\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlemma_\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msentence\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtoken\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_punct\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtoken\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_stop\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'is_punct'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WBaJ3ztZJxhP"
      },
      "source": [
        "# train word2vec on the the sentences\n",
        "model = gensim.models.Word2Vec(\n",
        "    sentences[\"text\"],\n",
        "    workers=4,\n",
        "    min_count=1,\n",
        "    window=6,\n",
        "    sg=0,\n",
        "    sample=1e-3,\n",
        "    size=100,\n",
        "    hs=1\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Udx13dzfJ1CO",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        },
        "outputId": "b81e078b-775a-459a-c630-782c20fcb2e7"
      },
      "source": [
        "print(model.most_similar(positive=['lady', 'man'], negative=['woman'], topn=5))\n",
        "print(model.doesnt_match(\"dad dinner mom aunt uncle\".split()))\n",
        "print(model.similarity('woman', 'man'))\n",
        "print(model.similarity('horse', 'cat'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('head', 0.9982106685638428), ('sort', 0.9980904459953308), ('meet', 0.9976276755332947), ('aunt', 0.997494637966156), ('handsome', 0.9972043633460999)]\n",
            "dinner\n",
            "0.99766195\n",
            "0.90391624\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AUvkszf-J4LP",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 441
        },
        "outputId": "d1736ad8-5bed-4a42-c6c9-43a6e895aff0"
      },
      "source": [
        "word2vec_arr = np.zeros((sentences.shape[0],100))\n",
        "\n",
        "for i, sentence in enumerate(sentences[\"text\"]):\n",
        "    word2vec_arr[i,:] = np.mean([model[lemma] for lemma in sentence], axis=0)\n",
        "\n",
        "word2vec_arr = pd.DataFrame(word2vec_arr)\n",
        "sentences = pd.concat([sentences[[\"author\", \"text\"]],word2vec_arr], axis=1)\n",
        "sentences.dropna(inplace=True)\n",
        "\n",
        "sentences.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>author</th>\n",
              "      <th>text</th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>...</th>\n",
              "      <th>60</th>\n",
              "      <th>61</th>\n",
              "      <th>62</th>\n",
              "      <th>63</th>\n",
              "      <th>64</th>\n",
              "      <th>65</th>\n",
              "      <th>66</th>\n",
              "      <th>67</th>\n",
              "      <th>68</th>\n",
              "      <th>69</th>\n",
              "      <th>70</th>\n",
              "      <th>71</th>\n",
              "      <th>72</th>\n",
              "      <th>73</th>\n",
              "      <th>74</th>\n",
              "      <th>75</th>\n",
              "      <th>76</th>\n",
              "      <th>77</th>\n",
              "      <th>78</th>\n",
              "      <th>79</th>\n",
              "      <th>80</th>\n",
              "      <th>81</th>\n",
              "      <th>82</th>\n",
              "      <th>83</th>\n",
              "      <th>84</th>\n",
              "      <th>85</th>\n",
              "      <th>86</th>\n",
              "      <th>87</th>\n",
              "      <th>88</th>\n",
              "      <th>89</th>\n",
              "      <th>90</th>\n",
              "      <th>91</th>\n",
              "      <th>92</th>\n",
              "      <th>93</th>\n",
              "      <th>94</th>\n",
              "      <th>95</th>\n",
              "      <th>96</th>\n",
              "      <th>97</th>\n",
              "      <th>98</th>\n",
              "      <th>99</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Carroll</td>\n",
              "      <td>[Alice, begin, tired, sit, sister, bank, have,...</td>\n",
              "      <td>0.156557</td>\n",
              "      <td>0.504342</td>\n",
              "      <td>-0.014885</td>\n",
              "      <td>0.312830</td>\n",
              "      <td>0.135841</td>\n",
              "      <td>0.052049</td>\n",
              "      <td>-0.431068</td>\n",
              "      <td>0.028523</td>\n",
              "      <td>0.272996</td>\n",
              "      <td>0.282735</td>\n",
              "      <td>-0.378080</td>\n",
              "      <td>0.067731</td>\n",
              "      <td>-0.031333</td>\n",
              "      <td>-0.053178</td>\n",
              "      <td>-0.099557</td>\n",
              "      <td>-0.107361</td>\n",
              "      <td>-0.398297</td>\n",
              "      <td>0.211715</td>\n",
              "      <td>0.367610</td>\n",
              "      <td>0.073099</td>\n",
              "      <td>-0.284089</td>\n",
              "      <td>0.439602</td>\n",
              "      <td>-0.084738</td>\n",
              "      <td>0.176117</td>\n",
              "      <td>0.003873</td>\n",
              "      <td>-0.007618</td>\n",
              "      <td>-0.475505</td>\n",
              "      <td>0.095376</td>\n",
              "      <td>-0.110146</td>\n",
              "      <td>-0.442957</td>\n",
              "      <td>-0.297493</td>\n",
              "      <td>0.045424</td>\n",
              "      <td>0.030883</td>\n",
              "      <td>0.215378</td>\n",
              "      <td>-0.294835</td>\n",
              "      <td>-0.116270</td>\n",
              "      <td>0.095659</td>\n",
              "      <td>-0.417053</td>\n",
              "      <td>...</td>\n",
              "      <td>0.507816</td>\n",
              "      <td>-0.240877</td>\n",
              "      <td>0.608371</td>\n",
              "      <td>-0.363942</td>\n",
              "      <td>0.272643</td>\n",
              "      <td>-0.150568</td>\n",
              "      <td>0.148696</td>\n",
              "      <td>0.008605</td>\n",
              "      <td>0.048195</td>\n",
              "      <td>-0.160269</td>\n",
              "      <td>0.475473</td>\n",
              "      <td>0.109559</td>\n",
              "      <td>0.313951</td>\n",
              "      <td>-0.166453</td>\n",
              "      <td>-0.134322</td>\n",
              "      <td>-0.159020</td>\n",
              "      <td>0.284053</td>\n",
              "      <td>0.159858</td>\n",
              "      <td>-0.301681</td>\n",
              "      <td>-0.053141</td>\n",
              "      <td>0.371239</td>\n",
              "      <td>-0.263162</td>\n",
              "      <td>-0.173714</td>\n",
              "      <td>0.017265</td>\n",
              "      <td>0.316261</td>\n",
              "      <td>-0.142782</td>\n",
              "      <td>-0.111847</td>\n",
              "      <td>-0.524318</td>\n",
              "      <td>-0.032784</td>\n",
              "      <td>0.370151</td>\n",
              "      <td>0.134086</td>\n",
              "      <td>0.125074</td>\n",
              "      <td>0.032564</td>\n",
              "      <td>0.191429</td>\n",
              "      <td>-0.062701</td>\n",
              "      <td>0.006702</td>\n",
              "      <td>-0.090056</td>\n",
              "      <td>0.283728</td>\n",
              "      <td>-0.000987</td>\n",
              "      <td>0.210049</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Carroll</td>\n",
              "      <td>[consider, mind, hot, day, feel, sleepy, stupi...</td>\n",
              "      <td>0.116939</td>\n",
              "      <td>0.401289</td>\n",
              "      <td>0.008777</td>\n",
              "      <td>0.262100</td>\n",
              "      <td>0.137854</td>\n",
              "      <td>0.043824</td>\n",
              "      <td>-0.355057</td>\n",
              "      <td>0.034746</td>\n",
              "      <td>0.207341</td>\n",
              "      <td>0.213598</td>\n",
              "      <td>-0.324004</td>\n",
              "      <td>0.044033</td>\n",
              "      <td>-0.039625</td>\n",
              "      <td>-0.060718</td>\n",
              "      <td>-0.087796</td>\n",
              "      <td>-0.068523</td>\n",
              "      <td>-0.322759</td>\n",
              "      <td>0.183491</td>\n",
              "      <td>0.299840</td>\n",
              "      <td>0.052520</td>\n",
              "      <td>-0.243587</td>\n",
              "      <td>0.374562</td>\n",
              "      <td>-0.066973</td>\n",
              "      <td>0.161563</td>\n",
              "      <td>0.025756</td>\n",
              "      <td>-0.027119</td>\n",
              "      <td>-0.354464</td>\n",
              "      <td>0.085367</td>\n",
              "      <td>-0.093945</td>\n",
              "      <td>-0.358640</td>\n",
              "      <td>-0.239173</td>\n",
              "      <td>0.050123</td>\n",
              "      <td>0.039068</td>\n",
              "      <td>0.164698</td>\n",
              "      <td>-0.233838</td>\n",
              "      <td>-0.093474</td>\n",
              "      <td>0.063087</td>\n",
              "      <td>-0.331597</td>\n",
              "      <td>...</td>\n",
              "      <td>0.422756</td>\n",
              "      <td>-0.197529</td>\n",
              "      <td>0.478835</td>\n",
              "      <td>-0.287188</td>\n",
              "      <td>0.205328</td>\n",
              "      <td>-0.121597</td>\n",
              "      <td>0.126237</td>\n",
              "      <td>-0.003829</td>\n",
              "      <td>0.032560</td>\n",
              "      <td>-0.122114</td>\n",
              "      <td>0.403670</td>\n",
              "      <td>0.089553</td>\n",
              "      <td>0.273978</td>\n",
              "      <td>-0.104055</td>\n",
              "      <td>-0.095724</td>\n",
              "      <td>-0.144091</td>\n",
              "      <td>0.234705</td>\n",
              "      <td>0.116129</td>\n",
              "      <td>-0.240797</td>\n",
              "      <td>-0.034599</td>\n",
              "      <td>0.301703</td>\n",
              "      <td>-0.231728</td>\n",
              "      <td>-0.144601</td>\n",
              "      <td>0.023942</td>\n",
              "      <td>0.259804</td>\n",
              "      <td>-0.134262</td>\n",
              "      <td>-0.073277</td>\n",
              "      <td>-0.417739</td>\n",
              "      <td>-0.039294</td>\n",
              "      <td>0.305030</td>\n",
              "      <td>0.107210</td>\n",
              "      <td>0.119562</td>\n",
              "      <td>0.018667</td>\n",
              "      <td>0.169521</td>\n",
              "      <td>-0.040365</td>\n",
              "      <td>0.002672</td>\n",
              "      <td>-0.089513</td>\n",
              "      <td>0.224778</td>\n",
              "      <td>0.009853</td>\n",
              "      <td>0.154534</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Carroll</td>\n",
              "      <td>[remarkable, Alice, think, way, hear, Rabbit, ...</td>\n",
              "      <td>0.167634</td>\n",
              "      <td>0.576445</td>\n",
              "      <td>-0.023443</td>\n",
              "      <td>0.350662</td>\n",
              "      <td>0.156439</td>\n",
              "      <td>0.046915</td>\n",
              "      <td>-0.511667</td>\n",
              "      <td>0.015956</td>\n",
              "      <td>0.297355</td>\n",
              "      <td>0.325010</td>\n",
              "      <td>-0.433335</td>\n",
              "      <td>0.081311</td>\n",
              "      <td>-0.051280</td>\n",
              "      <td>-0.058653</td>\n",
              "      <td>-0.120802</td>\n",
              "      <td>-0.133634</td>\n",
              "      <td>-0.473012</td>\n",
              "      <td>0.226878</td>\n",
              "      <td>0.415087</td>\n",
              "      <td>0.063371</td>\n",
              "      <td>-0.317654</td>\n",
              "      <td>0.513487</td>\n",
              "      <td>-0.121112</td>\n",
              "      <td>0.207148</td>\n",
              "      <td>0.011231</td>\n",
              "      <td>-0.004099</td>\n",
              "      <td>-0.545946</td>\n",
              "      <td>0.111926</td>\n",
              "      <td>-0.133873</td>\n",
              "      <td>-0.509538</td>\n",
              "      <td>-0.349998</td>\n",
              "      <td>0.055461</td>\n",
              "      <td>0.052263</td>\n",
              "      <td>0.249190</td>\n",
              "      <td>-0.344722</td>\n",
              "      <td>-0.113718</td>\n",
              "      <td>0.115739</td>\n",
              "      <td>-0.490992</td>\n",
              "      <td>...</td>\n",
              "      <td>0.609013</td>\n",
              "      <td>-0.272200</td>\n",
              "      <td>0.697808</td>\n",
              "      <td>-0.431249</td>\n",
              "      <td>0.324589</td>\n",
              "      <td>-0.165976</td>\n",
              "      <td>0.177741</td>\n",
              "      <td>0.002685</td>\n",
              "      <td>0.063447</td>\n",
              "      <td>-0.193253</td>\n",
              "      <td>0.559225</td>\n",
              "      <td>0.154242</td>\n",
              "      <td>0.357273</td>\n",
              "      <td>-0.176755</td>\n",
              "      <td>-0.141429</td>\n",
              "      <td>-0.187650</td>\n",
              "      <td>0.319641</td>\n",
              "      <td>0.187974</td>\n",
              "      <td>-0.345561</td>\n",
              "      <td>-0.057221</td>\n",
              "      <td>0.437245</td>\n",
              "      <td>-0.303590</td>\n",
              "      <td>-0.212949</td>\n",
              "      <td>0.025946</td>\n",
              "      <td>0.366633</td>\n",
              "      <td>-0.166108</td>\n",
              "      <td>-0.135306</td>\n",
              "      <td>-0.611266</td>\n",
              "      <td>-0.018768</td>\n",
              "      <td>0.437321</td>\n",
              "      <td>0.136005</td>\n",
              "      <td>0.135738</td>\n",
              "      <td>0.033673</td>\n",
              "      <td>0.192804</td>\n",
              "      <td>-0.053868</td>\n",
              "      <td>0.004824</td>\n",
              "      <td>-0.103954</td>\n",
              "      <td>0.334612</td>\n",
              "      <td>0.016020</td>\n",
              "      <td>0.255659</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Carroll</td>\n",
              "      <td>[oh, dear]</td>\n",
              "      <td>0.117552</td>\n",
              "      <td>0.453083</td>\n",
              "      <td>-0.012553</td>\n",
              "      <td>0.288845</td>\n",
              "      <td>0.145011</td>\n",
              "      <td>0.037289</td>\n",
              "      <td>-0.414303</td>\n",
              "      <td>0.021134</td>\n",
              "      <td>0.216287</td>\n",
              "      <td>0.246997</td>\n",
              "      <td>-0.351296</td>\n",
              "      <td>0.038186</td>\n",
              "      <td>-0.026018</td>\n",
              "      <td>-0.058227</td>\n",
              "      <td>-0.084984</td>\n",
              "      <td>-0.095092</td>\n",
              "      <td>-0.370731</td>\n",
              "      <td>0.177488</td>\n",
              "      <td>0.337987</td>\n",
              "      <td>0.080276</td>\n",
              "      <td>-0.228769</td>\n",
              "      <td>0.392130</td>\n",
              "      <td>-0.075595</td>\n",
              "      <td>0.185601</td>\n",
              "      <td>0.022833</td>\n",
              "      <td>-0.019594</td>\n",
              "      <td>-0.415328</td>\n",
              "      <td>0.114959</td>\n",
              "      <td>-0.093607</td>\n",
              "      <td>-0.400744</td>\n",
              "      <td>-0.266566</td>\n",
              "      <td>0.041614</td>\n",
              "      <td>0.060701</td>\n",
              "      <td>0.230814</td>\n",
              "      <td>-0.299062</td>\n",
              "      <td>-0.118340</td>\n",
              "      <td>0.096874</td>\n",
              "      <td>-0.422802</td>\n",
              "      <td>...</td>\n",
              "      <td>0.537509</td>\n",
              "      <td>-0.253867</td>\n",
              "      <td>0.620693</td>\n",
              "      <td>-0.392961</td>\n",
              "      <td>0.276104</td>\n",
              "      <td>-0.159166</td>\n",
              "      <td>0.176203</td>\n",
              "      <td>-0.013564</td>\n",
              "      <td>0.041083</td>\n",
              "      <td>-0.170205</td>\n",
              "      <td>0.527040</td>\n",
              "      <td>0.148760</td>\n",
              "      <td>0.335442</td>\n",
              "      <td>-0.142431</td>\n",
              "      <td>-0.126843</td>\n",
              "      <td>-0.188578</td>\n",
              "      <td>0.297223</td>\n",
              "      <td>0.149271</td>\n",
              "      <td>-0.295473</td>\n",
              "      <td>-0.036655</td>\n",
              "      <td>0.389080</td>\n",
              "      <td>-0.299601</td>\n",
              "      <td>-0.207005</td>\n",
              "      <td>0.033140</td>\n",
              "      <td>0.315626</td>\n",
              "      <td>-0.152214</td>\n",
              "      <td>-0.097407</td>\n",
              "      <td>-0.526059</td>\n",
              "      <td>-0.043960</td>\n",
              "      <td>0.388996</td>\n",
              "      <td>0.134394</td>\n",
              "      <td>0.144853</td>\n",
              "      <td>0.039601</td>\n",
              "      <td>0.194676</td>\n",
              "      <td>-0.072663</td>\n",
              "      <td>-0.015563</td>\n",
              "      <td>-0.118429</td>\n",
              "      <td>0.273512</td>\n",
              "      <td>0.019187</td>\n",
              "      <td>0.218594</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Carroll</td>\n",
              "      <td>[shall, late]</td>\n",
              "      <td>0.132013</td>\n",
              "      <td>0.389604</td>\n",
              "      <td>-0.014970</td>\n",
              "      <td>0.223953</td>\n",
              "      <td>0.091411</td>\n",
              "      <td>0.036065</td>\n",
              "      <td>-0.318990</td>\n",
              "      <td>0.005912</td>\n",
              "      <td>0.221112</td>\n",
              "      <td>0.227200</td>\n",
              "      <td>-0.282236</td>\n",
              "      <td>0.065079</td>\n",
              "      <td>-0.031074</td>\n",
              "      <td>-0.027161</td>\n",
              "      <td>-0.079436</td>\n",
              "      <td>-0.089805</td>\n",
              "      <td>-0.303489</td>\n",
              "      <td>0.160611</td>\n",
              "      <td>0.281339</td>\n",
              "      <td>0.032045</td>\n",
              "      <td>-0.227189</td>\n",
              "      <td>0.342244</td>\n",
              "      <td>-0.069774</td>\n",
              "      <td>0.126075</td>\n",
              "      <td>-0.023212</td>\n",
              "      <td>0.014454</td>\n",
              "      <td>-0.387502</td>\n",
              "      <td>0.057436</td>\n",
              "      <td>-0.088997</td>\n",
              "      <td>-0.336785</td>\n",
              "      <td>-0.240506</td>\n",
              "      <td>0.031752</td>\n",
              "      <td>0.001222</td>\n",
              "      <td>0.156880</td>\n",
              "      <td>-0.220549</td>\n",
              "      <td>-0.083942</td>\n",
              "      <td>0.072426</td>\n",
              "      <td>-0.320512</td>\n",
              "      <td>...</td>\n",
              "      <td>0.379720</td>\n",
              "      <td>-0.172435</td>\n",
              "      <td>0.444954</td>\n",
              "      <td>-0.267936</td>\n",
              "      <td>0.219412</td>\n",
              "      <td>-0.106071</td>\n",
              "      <td>0.097951</td>\n",
              "      <td>0.015859</td>\n",
              "      <td>0.042354</td>\n",
              "      <td>-0.122185</td>\n",
              "      <td>0.336965</td>\n",
              "      <td>0.073037</td>\n",
              "      <td>0.229740</td>\n",
              "      <td>-0.135299</td>\n",
              "      <td>-0.095866</td>\n",
              "      <td>-0.113266</td>\n",
              "      <td>0.202954</td>\n",
              "      <td>0.134579</td>\n",
              "      <td>-0.235676</td>\n",
              "      <td>-0.044369</td>\n",
              "      <td>0.281121</td>\n",
              "      <td>-0.177154</td>\n",
              "      <td>-0.113152</td>\n",
              "      <td>0.003199</td>\n",
              "      <td>0.237271</td>\n",
              "      <td>-0.101208</td>\n",
              "      <td>-0.114352</td>\n",
              "      <td>-0.401953</td>\n",
              "      <td>-0.013157</td>\n",
              "      <td>0.273492</td>\n",
              "      <td>0.097854</td>\n",
              "      <td>0.076843</td>\n",
              "      <td>0.018238</td>\n",
              "      <td>0.122903</td>\n",
              "      <td>-0.038498</td>\n",
              "      <td>0.014136</td>\n",
              "      <td>-0.041507</td>\n",
              "      <td>0.224709</td>\n",
              "      <td>0.005873</td>\n",
              "      <td>0.165873</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 102 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "    author  ...        99\n",
              "0  Carroll  ...  0.210049\n",
              "1  Carroll  ...  0.154534\n",
              "2  Carroll  ...  0.255659\n",
              "3  Carroll  ...  0.218594\n",
              "4  Carroll  ...  0.165873\n",
              "\n",
              "[5 rows x 102 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XWyhRyZSKAT9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228
        },
        "outputId": "7c5d052c-8bd6-49d0-a6f7-6d5ae5ef448c"
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "Y = sentences['author']\n",
        "X = np.array(sentences.drop(['text','author'], 1))\n",
        "\n",
        "# We split the dataset into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.4, random_state=123)\n",
        "\n",
        "# Models\n",
        "lr = LogisticRegression()\n",
        "rfc = RandomForestClassifier()\n",
        "gbc = GradientBoostingClassifier()\n",
        "\n",
        "lr.fit(X_train, y_train)\n",
        "rfc.fit(X_train, y_train)\n",
        "gbc.fit(X_train, y_train)\n",
        "\n",
        "print(\"----------------------Logistic Regression Scores----------------------\")\n",
        "print('Training set score:', lr.score(X_train, y_train))\n",
        "print('\\nTest set score:', lr.score(X_test, y_test))\n",
        "\n",
        "print(\"----------------------Random Forest Scores----------------------\")\n",
        "print('Training set score:', rfc.score(X_train, y_train))\n",
        "print('\\nTest set score:', rfc.score(X_test, y_test))\n",
        "\n",
        "print(\"----------------------Gradient Boosting Scores----------------------\")\n",
        "print('Training set score:', gbc.score(X_train, y_train))\n",
        "print('\\nTest set score:', gbc.score(X_test, y_test))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "----------------------Logistic Regression Scores----------------------\n",
            "Training set score: 0.750076103500761\n",
            "\n",
            "Test set score: 0.7611872146118721\n",
            "----------------------Random Forest Scores----------------------\n",
            "Training set score: 0.9917808219178083\n",
            "\n",
            "Test set score: 0.8068493150684931\n",
            "----------------------Gradient Boosting Scores----------------------\n",
            "Training set score: 0.8864535768645357\n",
            "\n",
            "Test set score: 0.8027397260273973\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "flyjaCWLKFr3"
      },
      "source": [
        "# Load Google's pre-trained Word2Vec model.\n",
        "model_pretrained = gensim.models.KeyedVectors.load_word2vec_format(\n",
        "    'https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz', binary=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K3y6QkUsKHGx"
      },
      "source": [
        "word2vec_arr = np.zeros((sentences.shape[0],300))\n",
        "\n",
        "for i, sentence in enumerate(sentences[\"text\"]):\n",
        "  try:\n",
        "    word2vec_arr[i,:] = np.mean([model_pretrained[lemma] for lemma in sentence], axis=0)\n",
        "  except KeyError:\n",
        "    word2vec_arr[i,:] = np.full((1,300), np.nan)\n",
        "    continue\n",
        "\n",
        "word2vec_arr = pd.DataFrame(word2vec_arr)\n",
        "sentences = pd.concat([sentences[[\"author\", \"text\"]],word2vec_arr], axis=1)\n",
        "sentences.dropna(inplace=True)\n",
        "\n",
        "print(\"Shape of the dataset: {}\".format(sentences.shape))\n",
        "sentences.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7dN15aQ8KKgP"
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "Y = sentences['author']\n",
        "X = np.array(sentences.drop(['text','author'], 1))\n",
        "\n",
        "# We split the dataset into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.4, random_state=123)\n",
        "\n",
        "# Models\n",
        "lr = LogisticRegression()\n",
        "rfc = RandomForestClassifier()\n",
        "gbc = GradientBoostingClassifier()\n",
        "\n",
        "lr.fit(X_train, y_train)\n",
        "rfc.fit(X_train, y_train)\n",
        "gbc.fit(X_train, y_train)\n",
        "\n",
        "print(\"----------------------Logistic Regression Scores----------------------\")\n",
        "print('Training set score:', lr.score(X_train, y_train))\n",
        "print('\\nTest set score:', lr.score(X_test, y_test))\n",
        "\n",
        "print(\"----------------------Random Forest Scores----------------------\")\n",
        "print('Training set score:', rfc.score(X_train, y_train))\n",
        "print('\\nTest set score:', rfc.score(X_test, y_test))\n",
        "\n",
        "print(\"----------------------Gradient Boosting Scores----------------------\")\n",
        "print('Training set score:', gbc.score(X_train, y_train))\n",
        "print('\\nTest set score:', gbc.score(X_test, y_test))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}